---
title: "Data Science Capstone - Week 2"
author: "Alexander Zhou"
date: "2/18/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.path = "./figures/")
library(knitr)
library(dplyr)
library(parallel)
library(stringi) 
library(ggplot2)
library(tm)
library(RWeka)
```

## Introduction
This is the milestone report of the Coursera Data Science Capstone Project. This document will cover the exploratory data analysis of the Swift data set and the natural language processing to tokenize n-grams as a first step toward building a predictive model.

## Dataset Information
The data sets are expected to be located in a parent directory called `Data`. Variables such as `dir.separator` should be changed below, depending on the platform. 
```{r locals}
dir.separator <- "/"
swift.dir <- paste("..","Data", sep = dir.separator)
swift.lang <- "en_US"
swift.path <- paste(swift.dir, dir.separator, swift.lang, dir.separator, sep = "")
set.seed(2501)
sample.size <- 0.01
options(mc.cores = detectCores())
grams.n <- 20
```

Gathering some basic properties of the data set and combining all information into a tabular output.  
```{r rawdata, cache = TRUE}
blogs <- readLines(paste(swift.path, swift.lang,".blogs.txt", sep = ""), 
                         encoding = "UTF-8", skipNul = TRUE)
news <- readLines(paste(swift.path, swift.lang,".news.txt", sep = ""), 
                        encoding = "UTF-8", skipNul = TRUE)
twitter <- readLines(paste(swift.path, swift.lang,".twitter.txt", sep = ""), 
                           encoding = "UTF-8", skipNul = TRUE)

raw.stat <- data.frame(
            FileName=c(paste(swift.lang,".blogs.txt", sep = ""),
                       paste(swift.lang,".news.txt", sep = ""),
                       paste(swift.lang,".twitter.txt", sep = "")),
            FileSizeMB=c(file.info(paste(swift.path, swift.lang,".blogs.txt", 
                                         sep = ""))$size/1024^2,
                         file.info(paste(swift.path, swift.lang,".news.txt", 
                                         sep = ""))$size/1024^2,
                         file.info(paste(swift.path, swift.lang,".twitter.txt", 
                                         sep = ""))$size/1024^2),
            t(rbind(sapply(list(blogs,news,twitter),stri_stats_general),
            wordCount=sapply(list(blogs,news,twitter),stri_stats_latex)[4,])))
kable(raw.stat)
```

A [list](https://www.freewebheaders.com/full-list-of-bad-words-banned-by-google/) of abusive words is used to filter out unwanted words form the database.
```{r abusive, cache = TRUE}
if (!file.exists("abusive-words.txt")){
  download.file("https://www.freewebheaders.com/wordpress/wp-content/uploads/full-list-of-bad-words-banned-by-google-txt-file.zip", "abusive-words.zip")
  unzip("abusive-words.zip")
  file.rename("full-list-of-bad-words-banned-by-google-txt-file_2013_11_26_04_53_31_867.txt","abusive-words.txt", showWarnings = FALSE)
  file.remove("abusive-words.zip")
}
words.abusive <- readLines("abusive-words.txt", encoding="latin1", warn=FALSE, skipNul=TRUE)
```

An analysis of the complete data set is not feasable at this point thus representative samples were used for further analysis. Creating the corpus file using a fraction of 1% of the data set as sample from all three sources. The sample data is then stripped of numbers, punctuations, stop words, abusive terms and whitespaces.
```{r datacleaning, cache = TRUE}
words.sample <- list(sample(blogs, length(blogs) * sample.size, replace=FALSE),
                     sample(news, length(news) * sample.size, replace=FALSE),
                     sample(twitter, length(twitter) * sample.size, replace=FALSE))

words.clean <- VCorpus(VectorSource(words.sample)) %>% 
  tm_map(removeNumbers) %>% 
  tm_map(removePunctuation) %>% 
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(removeWords, words.abusive) %>% 
  tm_map(stripWhitespace) %>% 
  tm_map(content_transformer(tolower)) %>%
  tm_map(PlainTextDocument)
```

The cleaned data can then be tokenized to generate n-grams. 
```{r tokenization, cache = TRUE}
tokenizer <- function (x = words.clean, token) {
  NGramTokenizer(x, Weka_control(min = token, max = token))
}
```

Gathering and plotting the most frequent Unigrams.
```{r 1-gram, cache = TRUE}
gram.1 <- data.frame(table(tokenizer(words.clean, 1)))
gram.1 <- gram.1[order(gram.1$Freq, decreasing = TRUE),]
colnames(gram.1) <- c("Word", "Freq")
gram.1 <- head(gram.1, grams.n)
ggplot(gram.1, aes(x=reorder(Word, -Freq), y = Freq)) + 
  geom_bar(stat="identity", aes(fill=Freq)) + 
  ggtitle("Unigrams") + ylab("Frequency") + theme(axis.title.x=element_blank()) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(plot.title = element_text(hjust = 0.5)) + 
  theme(plot.title = element_text(lineheight=.8, face="bold")) + 
  guides(fill=FALSE)
```

Same routine for the most frequent Bigrams.
```{r 2-gram, cache = TRUE}
gram.2 <- data.frame(table(tokenizer(words.clean, 2)))
gram.2 <- gram.2[order(gram.2$Freq, decreasing = TRUE),]
colnames(gram.2) <- c("Word", "Freq")
gram.2 <- head(gram.2, grams.n)
ggplot(gram.2, aes(x=reorder(Word, -Freq), y = Freq)) + 
  geom_bar(stat="identity", aes(fill=Freq)) + 
  ggtitle("Bigrams") + ylab("Frequency") + theme(axis.title.x=element_blank()) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(plot.title = element_text(hjust = 0.5)) + 
  theme(plot.title = element_text(lineheight=.8, face="bold")) + 
  guides(fill=FALSE)
```

Most frequent Trigrams tokenization and plotting. 
```{r 3-gram, cache = TRUE}
gram.3 <- data.frame(table(tokenizer(words.clean, 3)))
gram.3 <- gram.3[order(gram.3$Freq, decreasing = TRUE),]
colnames(gram.3) <- c("Word", "Freq")
gram.3 <- head(gram.3, grams.n)
ggplot(gram.3, aes(x=reorder(Word, -Freq), y = Freq)) + 
  geom_bar(stat="identity", aes(fill=Freq)) + 
  ggtitle("Trigrams") + ylab("Frequency") + theme(axis.title.x=element_blank()) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(plot.title = element_text(hjust = 0.5)) + 
  theme(plot.title = element_text(lineheight=.8, face="bold")) + 
  guides(fill=FALSE)
```

## Next Steps
Starting with this exploratory analysis, the next step would be building the predictive model using the token data and doing a frequency lookup. The final model should consider a bigger chunk of the data. The final algorithm will be tested and deployed as Shiny app. 

